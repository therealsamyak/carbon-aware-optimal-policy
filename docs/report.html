<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="noindex, nofollow" />
    <meta name="googlebot" content="noindex, nofollow" />
    <title>Report - Optimal Charge Security Camera</title>
    <link rel="stylesheet" href="styles.css" />
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"
    ></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>
      hljs.highlightAll();
    </script>
  </head>
  <body>
    <!-- Top Navigation -->
    <nav class="top-nav">
      <a href="index.html" class="logo">OCS</a>
      <div class="nav-links">
        <a href="index.html">Home</a>
        <a href="report.html" class="active">Report</a>
        <a href="docs.html">Documentation</a>
        <a href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2" target="_blank"
          >GitHub</a
        >
      </div>
    </nav>

    <div class="layout-container">
      <!-- Sidebar -->
      <aside class="sidebar">
        <h2>Table of Contents</h2>
        <ul class="toc-links">
          <li><a href="#abstract">Abstract</a></li>
          <li>
            <a href="#introduction">1. Introduction</a>
            <ul class="toc-sublinks">
              <li><a href="#motivation-objective">1.1 Motivation & Objective</a></li>
              <li>
                <a href="#state-of-art-limitations">1.2 State of the Art & Its Limitations</a>
              </li>
              <li><a href="#novelty-rationale">1.3 Novelty & Rationale</a></li>
              <li><a href="#potential-impact">1.4 Potential Impact</a></li>
              <li><a href="#challenges">1.5 Challenges</a></li>
              <li><a href="#metrics-success">1.6 Metrics of Success</a></li>
            </ul>
          </li>
          <li>
            <a href="#related-work">2. Related Work</a>
            <ul class="toc-sublinks">
              <li>
                <a href="#carbon-aware-large-scale"
                  >2.1 Carbon-Aware Computing in Large-Scale Systems</a
                >
              </li>
              <li><a href="#planning-control-methods">2.2 Planning and Control Methods</a></li>
              <li><a href="#other-related-works">2.3 Other Related Works</a></li>
            </ul>
          </li>
          <li>
            <a href="#technical-approach">3. Technical Approach</a>
            <ul class="toc-sublinks">
              <li><a href="#model-profiler">3.1 Model Profiler</a></li>
              <li><a href="#system-parameters">3.2 System Parameters (per Controller)</a></li>
              <li><a href="#oracle-controller">3.3 Oracle Controller</a></li>
              <li>
                <a href="#custom-controller">3.4 Custom Controller (POMDP, Imitation Learning)</a>
              </li>
              <li><a href="#naive-controller">3.5 Naive Controller</a></li>
              <li>
                <a href="#simulation">3.6 Simulation Setup</a>
              </li>
            </ul>
          </li>
          <li><a href="#evaluation-results">4. Evaluation & Results</a></li>
          <li>
            <a href="#discussion-conclusions">5. Discussion & Conclusions</a>
            <ul class="toc-sublinks">
              <li><a href="#worked-well">5.1 What worked well and why?</a></li>
              <li><a href="#didnt-work">5.2 What didn't work and why?</a></li>
              <li><a href="#limitations">5.3 Limitations</a></li>
              <li><a href="#future_work">5.4 Future Work</a></li>
            </ul>
          </li>
          <li><a href="#references">6. References</a></li>
          <li>
            <a href="#supplementary-material">7. Supplementary Material</a>
            <ul class="toc-sublinks">
              <li><a href="#datasets">7.1 Datasets</a></li>
              <li><a href="#software">7.2 Software</a></li>
            </ul>
          </li>
        </ul>

        <div class="footer-info">
          <p><strong>Contributors:</strong><br />Samyak Kakatur, Jasper Lin</p>
        </div>
      </aside>

      <!-- Main Content -->
      <main class="main-content">
        <div class="content-wrapper">
          <h1>Optimal Charge Security Camera</h1>
          <p>Main report for this project.</p>

          <!-- Abstract Section -->
          <section id="abstract" class="section">
            <h2>Abstract</h2>
            <p>
              Carbon awareness is becoming increasingly important in the modern world, and many
              companies are striving to be carbon-neutral in their operations. Existing research
              into carbon-aware computing has focused on high-power, high-performance computing
              systems, but there has not been any awareness into carbon-aware battery charging,
              especially for small-scale IoT devices. In this paper, we propose a controller
              framework for a battery-powered security camera that balances the tradeoff
              betweenaccuracy, latency, carbon footprint, and charging decisions based on a
              pre-defined reward function. This controller picks the optimal models and when to
              charge every task interval over a horizon of time.
            </p>
          </section>

          <!-- Introduction Section -->
          <section id="introduction" class="section">
            <h2>1. Introduction</h2>

            <h3 id="motivation-objective">1.1 Motivation & Objective</h3>
            <p>
              While significant research has addressed carbon-aware computing in data centers and
              high-performance systems, small-scale IoT devices remain overlooked. Battery-powered
              devices with flexible charging represent a massive, untapped opportunity for carbon
              optimization. If they are plugged in all the time, they can charge even when it is not
              carbon-efficient to do so.
            </p>
            <p>
              Our objective is to develop an intelligent controller that optimizes both
              computational (which model to select) and charging decisions for battery-powered
              security cameras. With this approach, we can charge when it is more carbon-efficient,
              rather than always charging regardless of the carbon cost. We balance our model
              selection to account for this carbon-efficiency
            </p>
            <p>
              Because there are differences of opinion on how much each factor (accuracy, latency,
              carbon footprint, camera uptime) matters to an individual, we define a specific reward
              function that balances these factors. The system supports dynamic weights, so if the
              initial weights are not balanced to a specifc use-case, they are adjustable for other
              use-cases.
            </p>

            <p>
              For a full breakdown of how we restricted the optimization to solve this problem, see
              Section 3.
            </p>

            <h3 id="state-of-art-limitations">1.2 State of the Art & Its Limitations</h3>
            <p>
              Current carbon-aware research is focused around data-center energy considerations.
              Data centers incur large electricity costs and carbon emissions. There are 2 main
              works related to this:
            </p>
            <ul>
              <li>
                Carbon- and Precedence-Aware Scheduling for Data Processing Clusters<sup
                  id="cite-ref-2"
                  ><a href="#cite-note-2">[2]</a></sup
                >
                - This paper is focused on scheduling data center workloads such that time
                requirements are met, but we delay lower priority tasks to carbon-efficient times.
              </li>
              <li>
                Carbon-Aware Workload Management in Data Centers<sup id="cite-ref-3"
                  ><a href="#cite-note-3">[3]</a></sup
                >
                - This paper is focused on integrating energy components besides the grid, such as
                cooling, heating, solar panels, batteries, energy storage, heat pumps, and heating
                connections to maintain existing data-center workloads while minimizing carbon
                emissions.
              </li>
            </ul>

            <p>
              Both of these papers focus on large scale systems with large scale workloads. These
              approachs are not directly applicable to small-scale IoT devices, which have very
              limited bandwitdh and very small battery capacity (energy storage).
            </p>

            <h3 id="novelty-rationale">1.3 Novelty & Rationale</h3>
            <p>
              Our approach relies on imitating an Oracle controller. This Oracle controller can see
              into the future and has full knowledge of the carbon timeseries before it occurs.
              Using that knowledge, we can solve the full discrete MDP based on the system state
              over a given finite horizon of time. We can then train a neural network to imitate
              this Oracle Controller, giving us a controller that can make decisions in real-time.
            </p>

            <p>
              Additionally, as carbon energy generally follows a sinusoidal pattern, we can leverage
              this neural network to also gain insight on the ebb and flow of carbon timeseries,
              further giving us more optimization potential.
            </p>

            <p>
              Finally, by modelling this problem as an MDP (and POMDP), we can take advantage of
              existing solving techniques (ex. Data-driven Planning via Imitation Learning<sup
                id="cite-ref-4"
                ><a href="#cite-note-4">[4]</a></sup
              >), which are well-studied and have multiple proven solutions.
            </p>

            <h3 id="potential-impact">1.4 Potential Impact</h3>
            <p>
              This system could easily be extended to other battery-powered devices that make
              decisions. Any system with a form of energy storage and energy input can take
              advantage of our models to create their own controllers that balance clean energy.
            </p>

            <h3 id="challenges">1.5 Challenges</h3>
            <p>
              Implementing an energy-aware decision-making controller for battery-powered edge
              devices presents several challenges. The primary difficulty is solving and generating
              sufficient training data from the Oracle MDP, as the state and action spaces grow
              quickly even under coarse discretization. Specific examples include:
            </p>
            <ul>
              <li>
                Deciding how static or dynamic the system should be to user-input. We can allow the
                user to dynamically adjust their requirements during runtime, but adjusting an
                already trained controller could be time-consuming, complex, or expensive,
                especially as we still have to maintain our task interval.
              </li>
              <li>
                Discretization of continuous variables. More precision means more runtime, but too
                much precision can make simple calculations more complex due to floating point
                errors.
              </li>
              <li>
                Carbon patterns vary from region to region. As such, we need to ensure as vast of
                variety carbon data as possible from different times of the year for a complete
                picture.
              </li>
            </ul>

            <h3 id="metrics-success">1.6 Metrics of Success</h3>
            <p>
              We compare our trained Imitation (Custom) Controller to the Oracle controller over the
              following metrics:
            </p>
            <ul>
              <li>
                Accuracy - how many times over the horizon does the model selected meet the user's
                requirements?
              </li>
              <ul>
                <li>
                  Success - number of times the selected model met the user's requirements, and had
                  enough energy to run.
                </li>
                <li>
                  Small Miss - number of times the selected model didn't meet the user's
                  requirements, and had enough energy to run.
                </li>
                <li>
                  Failure - number of times a model was unable to run, due to a lack of energy.
                </li>
              </ul>
              <li>
                Utility - differences in the rewards of the Oracle Controller (<a
                  href="#oracle-controller"
                  >Section 3.3</a
                >), the Imitation (Custom) Controller (<a href="#custom-controller">Section 3.4</a
                >), and Naive Controller (<a href="#naive-controller">Section 3.5</a>) decisions
                over each horizon.
              </li>
              <li>
                "Feasibility-Normalized Effective Uptime" - at each timestep \( t \), let \( F_t \)
                be the set of models feasible under the current energy constraint, and let \( a_t^*
                = \max_{m \in F_t} a(m) \) be the best achievable accuracy. If a model \( m_t \) is
                selected and runs, the timestep is scored as \( \frac{a(m_t)}{a_t^*} \); if no model
                is ran (Failure) it scores a \( 0 \). The metric is computed as the average score
                over the horizon.
              </li>
            </ul>
          </section>

          <!-- Related Work Section -->
          <section id="related-work" class="section">
            <h2>2. Related Work</h2>

            <h3 id="carbon-aware-large-scale">2.1 Carbon-Aware Computing in Large-Scale Systems</h3>
            <p>
              <strong>Carbon- and Precedence-Aware Scheduling for Data Processing Clusters</strong
              ><sup id="cite-ref-2"><a href="#cite-note-2">[2]</a></sup> developed scheduling
              algorithms that delay lower-priority data center workloads to carbon-efficient times
              while meeting time requirements. The authors focused on large-scale data processing
              clusters with significant energy consumption. While their approach successfully
              reduces carbon emissions in data centers, it assumes substantial energy storage and
              computational resources unavailable to small IoT devices.
            </p>
            <p>
              <strong>Carbon-Aware Workload Management in Data Centers</strong
              ><sup id="cite-ref-3"><a href="#cite-note-3">[3]</a></sup> proposed integrating
              multiple energy components including solar panels, batteries, and energy storage
              systems to minimize carbon emissions while maintaining data center workloads. Their
              multi-energy integration approach is effective for large facilities but requires
              complex infrastructure and significant capital investment, making it unsuitable for
              individual IoT devices.
            </p>

            <h3 id="planning-control-methods">2.2 Planning and Control Methods</h3>
            <p>
              <strong>Data-driven Planning via Imitation Learning</strong
              ><sup id="cite-ref-4"><a href="#cite-note-4">[4]</a></sup> introduced a framework for
              training neural networks to imitate optimal planners in complex decision-making
              problems. This paper specifically addresses using imitation learning to solve POMDPs,
              which is directly relevant to our problem. However, it focuses on robotic planning and
              heuristic search in spatial domains, whereas our work applies imitation learning to
              carbon-aware scheduling and resource-constrained model selection in energy systems.
            </p>

            <h3 id="other-related-works">2.3 Other Related Works</h3>
            <p>
              Other works in Section VI are not as directly related to this problem as the other
              papers above are, but still give valuable insight into applications of POMDPs.
            </p>
          </section>

          <!-- Technical Approach Section -->
          <section id="technical-approach" class="section">
            <h2>3. Technical Approach</h2>

            <h3 id="model-profiler">3.1 Model Profiler</h3>

            To simplify the optimization, we benchmarked and created model profiles separately
            before our simulation loop. The main simulation loop then uses these pre-computed
            profiles to make decisions at runtime.
            <b
              >We do NOT separately profile during the simulation itself, which is a known
              limitation.</b
            >
            See <a href="#limitations">Section 5.3</a> for more information.

            <h4>YOLOv10 Power and Latency Measurement</h4>
            <p>
              A Model task is defined as loading an image and running the default classifier
              function on it. The task involves:
            </p>

            <p>
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/model-profiler/power_profiler.py"
                target="_blank"
                >model-profiler/power_profiler.py:277</a
              >
            </p>
            <pre><code>start_time ← current_time()

FOR i ← 1 TO iterations DO
    avg_power ← measure_with_powermetrics(inference_task)
    append avg_power to inference_powers
    sleep for 4 seconds
END FOR

end_time ← current_time()
total_duration ← end_time - start_time

total_delay_time ← iterations x 4.0
actual_inference_duration ← total_duration - total_delay_time
avg_inference_time_seconds ← actual_inference_duration / iterations
</code></pre>

            <p>During this task, the power profiler measures:</p>
            <ul>
              <li>
                <strong>Power consumption</strong>: Baseline, idle, and inference power in
                milliwatts using macOS
                <a href="https://ss64.com/mac/powermetrics.html" target="_blank">powermetrics</a>
              </li>
              <li>
                <strong>Energy efficiency</strong>: Energy per inference in mWh calculated from
                power draw and inference time
              </li>
              <li>
                <strong>Performance metrics</strong>: Inference time, success rate, detection count
              </li>
              <li><strong>Model variants</strong>: All YOLOv10 versions (N, S, M, B, L, X)</li>
            </ul>

            <p>
              The profiler uses <code>powermetrics</code> with sudo access to capture CPU/GPU/ANE
              power consumption during the model task, storing results in
              <code>power_profiles.json</code>. Each model is benchmarked across 1000 iterations
              with outlier removal for stable baseline measurements. Time per inference is tracked
              to store latency information alongside power data. Outlier removal trims the top and
              bottom 20% of power readings to eliminate anomalies and uses median values for robust
              statistics.
            </p>

            <h4>YOLOv10 Accuracy</h4>
            <p>
              Model accuracy values are extracted from the COCO dataset benchmark results stored in
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/model-profiler/model-data.csv"
                target="_blank"
                >model-data.csv</a
              >. The dataset contains COCO mAP 50-95 scores for each YOLOv10 model variant, provided
              by
              <a href="https://docs.ultralytics.com/models/yolov10/" target="_blank"
                >Ultralytics official documentation</a
              >.
            </p>

            <p>
              The accuracy analysis processes the CSV data to provide approximate accuracy values
              for each YOLOv10 model version, enabling the controller to make informed tradeoffs
              between detection accuracy and energy consumption. We normalize these values by
              performing \( \text{NormalizedAccuracy} = \text{COCOmAP5095} / 57.0 \) to get a value
              between \([0, 1]\) for all models.
            </p>

            <h3 id="system-parameters">3.2 System Parameters (per Controller)</h3>

            <p>Let the system parameters be:</p>

            <p>
              \[ \Theta = (B_{\max}, r_{\text{chg}}, \Delta, T, \{d_t\}_{t=0}^{T-1}, u_{\text{acc}},
              u_{\text{lat}}, \mathcal{M}, E(m), W, X, Y, Z) \]
            </p>

            <p>Where:</p>

            <ul>
              <li>\(B_{\max}\) (mWh): maximum battery capacity</li>
              <li>\(\Delta\) (seconds): task interval (e.g. 300s)</li>
              <li>\(T\) (seconds): horizon length (e.g. 86400s)</li>
              <li>\(d_t \in [0,1]\): dirty energy fraction at timestep \(t\)</li>
              <li>
                \(\mathcal{M}\): model profile set, where each profile is \((m_{\text{acc}},
                m_{\text{lat}})\) with:
                <ul>
                  <li>
                    \(m_{\text{acc}} \in [0,1]\): model accuracy, where \(\text{MAX\_ACC} = \max_{m
                    \in \mathcal{M}} m_{\text{acc}}\)
                  </li>
                  <li>
                    \(m_{\text{lat}}\) (seconds): model latency, where \(\text{MIN\_LAT} = \min_{m
                    \in \mathcal{M}} m_{\text{lat}}\)
                  </li>
                </ul>
              </li>
              <li>
                \(E(m)\): energy function that maps model \(m\) to its energy consumption in mWh,
                where \(\text{ENERGY\_MIN} = \min_{m \in \mathcal{M}} E(m)\)
              </li>
              <li>
                \(u = (u_{\text{acc}}, u_{\text{lat}}) \in U = \{(u_{\text{acc}}, u_{\text{lat}})
                \in \mathbb{R}^2 \mid u_{\text{acc}} &lt; \text{MAX\_ACC}, u_{\text{lat}} &gt;
                \text{MIN\_LAT}\}\): user requirements, where \(u_{\text{acc}} \in [0,1]\) is the
                accuracy requirement, and \(u_{\text{lat}}\) (seconds) is the latency requirement
              </li>
              <li>\(r_{\text{chg}}\) (mWh/s): battery energy added per second</li>
              <li>
                \(W, X, Y, Z > 0\): reward function weights representing user preferences:
                <ul>
                  <li>\(W\): weight for running a model that successfully meets requirements</li>
                  <li>
                    \(X\): penalty weight for "small misses" (selected model successfully executed,
                    but it did not meet the accuracy or latency requirements)
                  </li>
                  <li>
                    \(Y\): penalty weight for "large misses" (no model was selected due to
                    insufficient battery capacity)
                  </li>
                  <li>\(Z\): penalty weight for amount of dirty energy consumed</li>
                </ul>
              </li>
            </ul>

            <b
              >To simplify this problem, we assume that ALL these parameters are constant per
              controller. If any of these parameters change, the controller will need to be solved
              and retrained again. See <a href="#limitations">Section 5.3</a> for more
              information.</b
            >

            <h3 id="oracle-controller">3.3 Oracle Controller</h3>
            <p>
              The oracle controller has full knowledge of the future carbon trajectory
              \(\{d_t\}_{t=0}^{T-1}\) over the planning horizon.
            </p>

            <h4>State Space</h4>
            <p>
              \[ {s} = (t, B_t) \in \mathcal{S} = \{(t, B_t) \mid t \in \{0,\ldots,T-1\},\; B_t \in
              [0, B_{\max}]\} \]
            </p>
            <p>
              Where \(t\) denotes the current timestep index and \(B_t\) (mWh) denotes the current
              battery level.
            </p>
            <p>
              This state representation is minimal: it includes exactly the variables that affect
              future feasibility and decision-making. In particular, the oracle's knowledge of the
              carbon trajectory is encoded implicitly through the timestep index \(t\), as the dirty
              energy fraction \(d_t\) is a known exogenous function of time.
            </p>

            <h4>Action Space</h4>
            <p>
              \[ a_t = (m_t, c_t) \in \mathcal{A} = (\mathcal{M} \cup \{\varnothing\}) \times
              \{0,1\} \]
            </p>
            <p>
              Each action is \(a_t = (m_t, c_t)\), where \(m_t\) is the model selected for execution
              (or \(\varnothing\) if no model is executed), and \(c_t\) indicates whether the system
              charges during the current task interval.
            </p>

            <h4>Transition Probability</h4>
            <p>
              Given state \(s_t = (t, B_t)\) and action \(a_t = (m_t, c_t)\), the transition
              probability to successor state \(s_{t+1} = (t', B')\) is defined as:
            </p>
            <p>
              \[ P(s_{t+1} \mid s_t, a_t) = \begin{cases} 1 & \text{if } t' = t + 1, B_t + c_t \cdot
              r_{\text{chg}} \ge E(m_t), B' = \min\!\left(B_{\max},\, B_t + c_t \cdot r_{\text{chg}}
              - E(m_t)\right)\\[6pt] 0 & \text{otherwise.} \end{cases} \]
            </p>
            <p>
              Thus, all feasible actions induce a deterministic transition with probability one,
              while actions that would result in negative battery energy are assigned zero
              probability and are therefore infeasible.
            </p>

            <h4>Reward Function</h4>
            <p>
              At each timestep, the environment produces outcome indicators based on the chosen
              action:
            </p>
            <ul>
              <li>
                \( \text{success}_t = \mathbf{1}\{m_t \neq \varnothing \wedge \text{acc}(m_t) \ge
                u_{\text{acc}} \wedge \text{lat}(m_t) \le u_{\text{lat}}\} \)
              </li>
              <li>
                \( \text{small\_miss}_t = \mathbf{1}\{m_t \neq \varnothing \wedge (\text{acc}(m_t) <
                u_{\text{acc}} \vee \text{lat}(m_t) > u_{\text{lat}})\} \)
              </li>
              <li>\( \text{large\_miss}_t = \mathbf{1}\{m_t = \varnothing\} \)</li>
            </ul>
            <p>The dirty energy incurred at timestep \(t\) is:</p>
            <p>\[ \Delta D_t = c_t \cdot r_{\text{chg}}\cdot \Delta \cdot d_t. \]</p>
            <p>
              Where \(c_t\) is the charge decision (0 or 1), \(r_{\text{chg}}\) is the charging
              rate, \(\Delta\) is the task interval, and \(d_t\) is the percent of energy that is
              dirty at that time.
            </p>
            <p>The immediate reward is defined as:</p>
            <p>
              \[ R(s_t,a_t,s_{t+1}) = W \cdot \text{success}_t - X \cdot \text{small\_miss}_t - Y
              \cdot \text{large\_miss}_t - Z \cdot \Delta D_t. \]
            </p>
            <p>
              The optimization objective is to maximize the <em>cumulative</em> reward over the
              planning horizon. As a result, the total contribution of successes, misses, and carbon
              cost depends only on their aggregate counts and total dirty energy consumed, not on
              the specific timesteps at which they occur. In particular, for fixed carbon intensity,
              a success followed by a miss yields the same cumulative reward as a miss followed by a
              success, and the timing of a success within the horizon does not affect its
              contribution as long as feasibility is maintained.
            </p>
            <p>
              Cumulative quantities such as total successes, misses, or dirty energy consumed are
              <em>not</em> included in the state. These quantities are only used to compute the
              cumulative sum of per-step rewards and do not influence future system dynamics or
              feasibility. Because carbon cost is modeled as a soft penalty and does not constrain
              future actions, histories that reach the same state \((t,B_t)\) differ only by an
              additive constant in accumulated reward. Consequently, the optimal continuation from a
              given state depends solely on the remaining horizon and current battery level,
              preserving the Markov property without explicitly storing historical metrics in the
              state.
            </p>

            <h4>Objective</h4>
            <p>The oracle policy \(\pi^*\) maximizes the cumulative reward over the horizon:</p>
            <p>\[ \pi^* = \arg\max_{\pi} \sum_{t=0}^{T-1} R(s_t, \pi(s_t), s_{t+1}). \]</p>
            <p>This optimization is solved exactly using finite-horizon dynamic programming.</p>
            <ul>
              <li>
                <b>Policy Lookup with K-Nearest Neighbors:</b>
                <ul>
                  <li>
                    The simulation uses battery discretization with \(F \in \mathbb{R}\) mWh steps
                    for computational efficiency in dynamic programming, while maintaining
                    continuous battery values in state transitions.
                  </li>
                  <li>
                    The simulation uses a K-nearest neighbor approach with parameter \(K \in
                    \mathbb{N}_{odd}, K > 0\) to find the nearest discretized battery level with
                    feasible actions. The algorithm identifies the K closest battery levels and
                    selects the first one that has valid feasible actions. If fewer than K feasible
                    options are found within the initial search radius, the algorithm expands the
                    search radius incrementally until exactly K valid options are identified. This
                    ensures robust policy extraction while maintaining optimality within the
                    expanded search space.
                  </li>
                </ul>
              </li>
            </ul>

            <p><strong>Basic DP Algorithm with K-NN:</strong></p>
            <pre><code>FOR t ← T-1 DOWNTO 0 DO
    FOR each battery_level b IN reachable_states DO
        best_value ← -∞
        best_action ← null
        
        FOR each action a IN feasible_actions(b) DO
            next_state ← transition(t, b, a)
            reward ← R(t, b, a, next_state)
            future_value ← V[t+1][knn(battery_key(next_state))]
            total_value ← reward + future_value
            
            IF total_value > best_value THEN
                best_value ← total_value
                best_action ← a
            END IF
        END FOR
        
        V[t][battery_key(b)] ← best_value
        π[t][battery_key(b)] ← best_action
    END FOR
END FOR
</code></pre>

            <p>
              The oracle controller implementation can be found in
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/simulation/controllers/oracle.py"
                target="_blank"
                >oracle.py</a
              >.
            </p>

            <h3 id="custom-controller">3.4 Custom Controller (POMDP, Imitation Learning)</h3>

            <p>
              The custom controller operates without access to the future carbon trajectory
              \(\{d_t\}_{t=0}^{T-1}\) available to the oracle (<a href="#oracle-controller"
                >Section 3.3</a
              >). At each timestep, it must make decisions based only on real-time observations. As
              a result, the control problem is naturally modeled as a Partially Observable Markov
              Decision Process (POMDP).
            </p>

            <h4>State Space, Action Space, Transition Probability, and Reward Function</h4>

            <p>
              The state space \(\mathcal{S}\), action space \(\mathcal{A} = (\mathcal{M} \cup
              \{\varnothing\}) \times \{0,1\}\), transition probability \(P(s_{t+1} \mid s_t,
              a_t)\), and reward function \(R(s_t, a_t, s_{t+1})\) are defined exactly as in
              <a href="#oracle-controller">Section 3.3</a>. The custom controller operates in the
              same underlying MDP; only its observability differs.
            </p>

            <h4>Observation Space</h4>

            <p>At each timestep \(t\), the controller receives the observation:</p>

            <p>
              \[ o_t = (B_t, d_t, \Delta d_t) \in \mathcal{O} = [0, B_{\max}] \times [0,1] \times
              [-1,1], \]
            </p>

            <p>
              where \(B_t\) is the current battery level, \(d_t\) is the instantaneous dirty energy
              fraction, and \(\Delta d_t = d_t - d_{t-1}\) captures the change in carbon intensity
              since the previous timestep. The controller does not observe the timestep index \(t\)
              or any future carbon values \(\{d_{t'}\}_{t' > t}\).
            </p>

            <p>
              The inclusion of \(\Delta d_t\) provides a minimal trend signal, allowing the
              controller to infer short-term carbon dynamics using only locally available
              information. Unlike the oracle, which implicitly accesses the full trajectory through
              \(t\), the custom controller must reason about carbon evolution from this limited
              window.
            </p>

            <h4>Policy Learning via Imitation</h4>

            <p>
              Rather than attempting to solve the POMDP via belief-state planning, we learn the
              custom controller via imitation learning using the oracle policy \(\pi^*\) (<a
                href="#oracle-controller"
                >Section 3.3</a
              >) as an expert.
            </p>

            <b
              >To simplify this problem, we assume that ALL system parameters in
              <a href="#system-parameters">Section 3.2</a> are constant per controller. If any of
              these parameters change, the controller will need to be solved and retrained again.
              See <a href="#limitations">Section 5.3</a> for more information.</b
            >

            <h4 id="training-data-generation">Training Data Generation</h4>
            <p>
              To create training data for the custom controller, the oracle solver's optimal
              decisions must be translated into observation-action pairs that the imitation learning
              algorithm can learn from. This process extracts the oracle's state-action trajectories
              and converts them into the observation space that the custom controller will actually
              encounter at runtime.
            </p>

            <p>The training data generation process works as follows:</p>
            <pre><code>FOR each optimal trajectory τ FROM oracle solver DO
    observations ← []
    actions ← []
    
    FOR each timestep t IN τ DO
        state, action ← τ[t]
        
        // Extract carbon information
        carbon_intensity ← carbon_data[t]  // [0,1]
        carbon_change ← carbon_data[t] - carbon_data[t-1]  // [-1,1]
        
        // Normalize battery level for observation space
        normalized_battery ← state.battery_level / B_max  // [0,1]
        
        // Create observation vector (3D)
        observation ← [normalized_battery, carbon_intensity, carbon_change]
        
        // Encode action as discrete targets
        model_index ← index_of(action.model)  // 0-6 for YOLOv10 variants
        charge_decision ← int(action.charge)  // 0 or 1
        
        // Append to training dataset
        observations.append(observation)
        actions.append([model_index, charge_decision])
    END FOR
    
    // Save as compressed numpy arrays with metadata
    save_training_chunk(observations, actions, trajectory_metadata)
END FOR</code></pre>
            <p>
              Implementation:
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/data/oracle_runner.py"
                target="_blank"
                >data/oracle_runner.py:70</a
              >
              and
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/simulation/controllers/oracle.py"
                target="_blank"
                >simulation/controllers/oracle.py:504</a
              >
            </p>

            <ul>
              <li>
                <strong>Observation Space Translation:</strong> The oracle has full knowledge of the
                timestep \(t\) and future carbon trajectory, but the custom controller only observes
                \((B_t, d_t, \Delta d_t)\). The training data generation extracts exactly what the
                custom controller will see at runtime.
              </li>
              <li>
                <strong>Action Encoding:</strong> Model selection is encoded as discrete indices
                (0-6) corresponding to YOLOv10 variants, while charging decisions become binary
                targets (0/1).
              </li>
              <li>
                <strong>Normalization:</strong> Battery levels are normalized to \([0,1]\) to make
                the neural network training stable across different battery configurations.
              </li>
            </ul>

            <h4 id="neural-network-architecture-design">Neural Network Architecture Design</h4>
            <p>
              The custom controller uses a 3-layer feedforward network processing 3-dimensional
              observations: battery level, carbon intensity, and carbon change. The network outputs
              8 dimensions: 7 model logits + 1 charge logit.
            </p>

            <pre><code>PolicyNetwork Architecture:
Input Layer: 3 features
Hidden Layer 1: 64 neurons, ReLU activation
Hidden Layer 2: 32 neurons, ReLU activation  
Output Layer: 8 neurons (7 model + 1 charge)

Forward Pass:
shared_output = Sequential(Input → Linear(3,64) → ReLU → Linear(64,32) → ReLU → Linear(32,8))
model_logits = shared_output[:, :7]
charge_logit = shared_output[:, 7:]</code></pre>
            <p>
              Implementation:
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/training/train.py"
                target="_blank"
                >training/train.py</a
              >
            </p>

            <p>
              Model selection uses cross-entropy loss, charging decision uses binary cross-entropy
              loss. Final loss combines both with equal weighting.
            </p>

            <p>
              The architecture consists of multiple hidden layers with ReLU activations, culminating
              in a dual-head output structure. One head produces logits for discrete model selection
              across available YOLOv10 variants, while the other head generates a continuous signal
              for the charging decision. This design enables the network to learn separate
              representations for model selection and energy management strategies.
            </p>
            <p>
              We incorporate feasibility-aware inference design to ensure safety in critical
              applications. The network predictions are validated against system constraints before
              execution, with mechanisms to handle prediction failures gracefully.
            </p>

            <h4 id="per-controller-training-approach">Per-Controller Training Approach</h4>
            <p>
              <b>
                Each system configuration receives its own neural network rather than one
                generalized model.
              </b>
              This approach was selected to save time with training data generation. See
              <a href="#limitations">Section 5.3</a>
              for more information.
            </p>

            <pre><code>Training Algorithm:
FOR each controller configuration DO
    Initialize fresh PolicyNetwork
    Set optimizer = Adam(lr=0.001)
    Set criterion = CombinedLoss(0.5 * CE_model + 0.5 * BCE_charge)
    
    FOR epoch = 1 TO 100 DO
        Train on batches of size 32
        Evaluate on validation set
        IF val_loss doesn't improve for 10 epochs THEN
            Break (early stopping)
        END IF
    END FOR
    Save best checkpoint
END FOR</code></pre>

            <p>
              Implementation:
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/training/train.py"
                target="_blank"
                >training/train.py</a
              >
            </p>

            <p>
              Loss combines model selection cross-entropy and charging binary cross-entropy with
              equal (0.5 each) weighting. Early stopping uses patience of 10 epochs with minimum
              delta of 0.001.
            </p>

            <h4 id="integration-with-simulation-framework">
              Integration with Simulation Framework
            </h4>

            <pre><code>Inference Pipeline:
Load trained model checkpoint
Set model to eval mode
FOR each timestep DO
    obs = preprocess([battery_level, carbon_intensity, carbon_change])
    model_logits, charge_logit = model(obs)
    
    model_pred = argmax(model_logits)
    charge_pred = sigmoid(charge_logit) > 0.5
    
    IF prediction_infeasible THEN
        NO_MODEL, NO_CHARGE selected
    END IF
END FOR</code></pre>

            <p>
              Implementation:
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/simulation/controllers/ml_controller.py"
                target="_blank"
                >simulation/controllers/ml_controller.py</a
              >
            </p>

            <p>
              Preprocessing normalizes battery level to [0,1] and calculates carbon change.
              Postprocessing converts model prediction to YOLOv10 variant and applies threshold to
              charge decision.
            </p>

            <h3 id="naive-controller">3.5 Naive Controller</h3>
            <p>
              We implement a naive baseline controller with a fixed, myopic policy that ignores
              carbon intensity and does not optimize over the horizon. The controller operates under
              the same system dynamics as defined in
              <a href="#system-parameters">Section&nbsp;3.2</a>.
            </p>

            <h4>Policy</h4>
            <p>
              At each timestep \(t\), the naive controller selects an action \(a_t = (m_t, c_t)\) as
              follows:
            </p>
            <ol>
              <li>
                Select the lowest-energy model in \(\mathcal{M}\) that satisfies \((u_{\text{acc}},
                u_{\text{lat}})\). If sufficient battery energy is available, execute the model
                without charging.
              </li>
              <li>
                If insufficient battery energy is available to run that model, select the
                lowest-energy model in \(\mathcal{M}\) regardless of requirements and charge during
                the interval.
              </li>
              <li>
                If insufficient battery energy is available to execute any model, select no model
                and charge during the interval.
              </li>
            </ol>

            <p>
              The naive controller code can be found in
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/simulation/controllers/naive.py"
                target="_blank"
                >naive.py</a
              >.
            </p>

            <h3 id="simulation">3.6 Simulation Setup</h3>

            <h4>Configuration Management</h4>
            <p>
              Each custom controller is trained on a specific set of system parameters and cannot
              operate outside those constraints. The simumlation loads the custom controller
              corresponding to the system parameters to ensure fair comparison across all
              controllers. See Section 5.3 for more information.
            </p>

            <h4>Main Simulation Loop</h4>
            <p>
              For each unseen test day, the simulation runs all three controllers simultaneously
              using the same carbon intensity trace and initial conditions. Each controller makes
              decisions at every task interval based on its available information: the oracle has
              full future carbon knowledge, the naive controller uses only current battery state,
              and the custom controller uses real-time observations.
            </p>

            <pre><code>FOR timestep FROM 0 TO horizon_length-1 DO
    current_state ← (timestep, battery_level)
    
    FOR each controller IN controllers DO
        action ← controller.select_action(current_state)
        
        IF action.is_feasible(current_state) THEN
            next_state ← transition(current_state, action)
            reward ← calculate_reward(action, next_state)
            path[controller].add(current_state, action, reward)
        ELSE
            handle_infeasible_action(controller, current_state)
        END IF
    END FOR
    
    update_all_battery_states()
END FOR</code></pre>

            <h4>Result Aggregation</h4>
            <p>
              After completing the simulation horizon, results are aggregated for each controller.
              Performance metrics include total cumulative reward, success rates, charging patterns,
              and model usage statistics. The comparison evaluates how well the custom controller
              approximates oracle performance while respecting real-time constraints.
            </p>

            <pre><code>FOR each controller IN controllers DO
    total_reward ← sum(path[controller].rewards)
    success_rate ← count(path[controller].actions WHERE meets_requirements) / total_actions
    utility_metrics ← calculate_utility(path[controller])
    
    record_performance(controller, total_reward, success_rate, utility_metrics)
END FOR

generate_comparative_analysis(controllers)</code></pre>

            <p>
              Implementation:
              <a
                href="https://github.com/therealsamyak/ECM202A_2025Fall_Project_2/blob/main/simulation/run.py"
                target="_blank"
                >simulation/run.py</a
              >
            </p>

            <!-- <h3>3.1 System Architecture</h3>
            <p>Include a block diagram or pipeline figure.</p>

            <h3>3.2 Data Pipeline</h3>
            <p>Explain how data is collected, processed, and used.</p>

            <h3>3.3 Algorithm / Model Details</h3>
            <p>Use math, pseudocode, or diagrams as needed.</p>

            <h3>3.4 Hardware / Software Implementation</h3>
            <p>Explain equipment, libraries, or frameworks.</p>

            <h3>3.5 Key Design Decisions & Rationale</h3>
            <p>Describe the main design decisions you made.</p> -->
          </section>

          <!-- Evaluation & Results Section -->
          <section id="evaluation-results" class="section">
            <h2>4. Evaluation & Results</h2>
            <p>
              Present experimental results with clarity and professionalism.<br /><br />
              Include:
            </p>
            <ul>
              <li>Plots (accuracy, latency, energy, error curves)</li>
              <li>Tables (comparisons with baselines)</li>
              <li>
                Qualitative visualizations (spectrograms, heatmaps, bounding boxes, screenshots)
              </li>
              <li>Ablation studies</li>
              <li>Error analysis / failure cases</li>
            </ul>
            <p>Each figure should have a caption and a short interpretation.</p>

            <h3 id="parameters-used">4.1 Parameters Used</h3>
          </section>

          <!-- Discussion & Conclusions Section -->
          <section id="discussion-conclusions" class="section">
            <h2>5. Discussion & Conclusions</h2>
            <p>This should synthesize—not merely repeat—your results.</p>

            <h3 id="worked-well">5.1 What worked well and why?</h3>
            <br />

            <h3 id="didnt-work">5.2 What didn't work and why?</h3>
            <br />

            <h3 id="limitations">5.3 Limitations</h3>

            <p>
              Due to computational and project time constraints, our simulation engine has the
              following limitations:
            </p>
            <ul>
              <li>
                There is no runtime measurement of battery consumption per task; we only use the
                model profiles.
              </li>
              <li>
                Idle energy consumed by any controller is not considered. In the real world, there
                is some very small idle energy cost even when the controller is 'sleeping.'
              </li>
              <li>
                Energy and latency consumed when running the controller-selection algorithm (or
                running the NN to make decision) is not accounted for.
              </li>
            </ul>
            <p>A future simulation engine would consider all the limitations above.</p>

            <p>
              <b
                >As mentioned numerous times above, our custom controller is specifically trained on
                a set of system parameters. The custom controller does not support varying the
                system parameters without retraining the controller.</b
              >
              If we had more time to run the oracle on a wider range of system parameters, we could
              generate a massive dataset using our oracle that can train a controller to recognize
              the diverse requirements and parameter combinations that are possible. This would give
              us one general-use controller that is plug-and-play anywhere. Unfortunately, due to
              time constraints, we could only train a limited number of custom controllers with
              rigid parameters.
            </p>

            <p>
              Due to our time constraints, we were not able to perform more ablation studies on this
              controller. The computational cost of solving the MDP, even after the restrictions and
              trade-offs we made, there was not enough time to do this. There are so many system
              parameters that tuning a controller for every single combination, especially with the
              many tasks we were doing, is just not feasible. This ties in to the previous point, as
              more abelation studies would also allow for the general controller.
            </p>

            <h3 id="future_work">5.4 Future Work</h3>

            <p>
              Future work could incorporate algorithms and methodology from the Data-driven Planning
              via Imitation Learning paper<sup id="cite-ref-4"><a href="#cite-note-4">[4]</a></sup>
              to speed up our oracle training data generation. While our current approach uses basic
              supervised learning with cross-entropy loss, more sophisticated imitation learning
              techniques could improve controller performance and make the algorithms faster.
            </p>

            <p>
              If we wanted to be simplier, we could incorperate a dual-algorithm controller, where
              the main policy comes from the neural-network, but the fallback policy is the same as
              the Naive, to get the best of both worlds.
            </p>

            <p>
              Finally, we could also explore an always-learning LSTM-type controller that
              self-updates itself every task interval at runtime. This would allow a
              'set-it-and-forget-it' autonomous controller that doesn't need to get retrained
              periodically.
            </p>
          </section>

          <!-- References Section -->
          <section id="references" class="section">
            <h2>6. References</h2>

            <h3 id="cite-note-2">
              Carbon- and Precedence-Aware Scheduling for Data Processing Clusters<cite>[2]</cite>
            </h3>
            <p>
              Lechowicz, A., Shenoy, R., Bashir, N., Hajiesmaili, M., Wierman, A., & Delimitrou, C.
              Carbon- and Precedence-Aware Scheduling for Data Processing Clusters.
              arXiv:2502.09717, 2025.
              <a href="https://arxiv.org/abs/2502.09717" target="_blank">[Paper]</a>
            </p>

            <h3 id="cite-note-3">
              Carbon-Aware Workload Management in Data Centers<cite>[3]</cite>
            </h3>
            <p>
              Nkwawir, B.W., Kayalica, M.O., Guven, D., Duman, A.C., & Erden, H.S. Carbon-Aware
              Workload Management in Data Centers: A Multi-Energy Integration Approach. In
              Proceedings of 16th ACM International Conference on Future and Sustainable Energy
              Systems (E-Energy '25), Association for Computing Machinery, New York, NY, USA,
              907-914.
              <a href="https://doi.org/10.1145/3679240.3735104" target="_blank">[Paper]</a>
            </p>

            <h3 id="cite-note-4">Data-driven Planning via Imitation Learning<cite>[4]</cite></h3>
            <p>
              Choudhury, S., Bhardwaj, M., Arora, S., Kapoor, A., Ranade, G., Scherer, S., & Dey, D.
              Data-driven Planning via Imitation Learning. The Robotics Institute, Carnegie Mellon
              University & Microsoft Research.
              <a href="https://arxiv.org/abs/1711.06391" target="_blank">[Paper]</a>
            </p>

            <h3 id="cite-note-5">Monte-Carlo Planning in Large POMDPs<cite>[5]</cite></h3>
            <p>
              Silver, D., & Veness, J. Monte-Carlo Planning in Large POMDPs. MIT & UNSW, Sydney,
              Australia.
              <a
                href="https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/monte-carlo-planning-in-large-pomdps.pdf"
                target="_blank"
                >[Paper]</a
              >
            </p>

            <h3 id="cite-note-6">
              Optimal Control of Markov Processes with Incomplete State Information I<cite
                >[6]</cite
              >
            </h3>
            <p>
              Åström, K.J. Optimal Control of Markov Processes with Incomplete State Information I.
              In Journal of Mathematical Analysis and Applications 10. p.174-205, 1965.
              <a href="https://lup.lub.lu.se/record/8867084" target="_blank">[Paper]</a>
            </p>
          </section>

          <!-- Supplementary Material Section -->
          <section id="supplementary-material" class="section">
            <h2>7. Supplementary Material</h2>

            <h3 id="datasets">7.1 Datasets</h3>
            <p>
              <strong>Electricity Maps grid carbon traces (external).</strong> 2024 time-series CSVs
              at 5-minute granularity for 4 U.S. regions from
              <a href="https://www.electricitymaps.com/" target="_blank">Electricity Maps</a>,
              stored in energy-data/. We replay these traces in simulation as the time-varying
              carbon signal. Preprocessing: load CSV, sort by timestamp, select column 7
              (Carbon-free energy percentage, CFE%), and align to the simulator timestep (hold the
              most recent 5-minute value when the task interval is finer). No labeling.
            </p>
            <p>
              <strong>YOLOv10 model metadata (external).</strong> Per-variant accuracy and specs
              from
              <a href="https://docs.ultralytics.com/models/yolov10/" target="_blank"
                >Ultralytics YOLOv10 documentation</a
              >
              used to parameterize model trade-offs in the simulator. No labeling.
            </p>
            <p>
              <strong>Power profiling logs (internal).</strong> Our benchmark scripts generate
              per-model power and latency stats saved as JSON in results/ (for example
              power_profiles.json) and used to estimate energy-per-inference.
            </p>
            <p>
              <strong>Benchmark images (internal).</strong> Fixed images in benchmark-images/ used
              only to run consistent inference during profiling and simulation, not to train YOLO.
            </p>

            <h3 id="software">7.2 Software</h3>
            <p>
              <strong>External libraries:</strong>
              <a href="https://pytorch.org/" target="_blank">PyTorch</a>,
              <a href="https://numpy.org/" target="_blank">NumPy</a>,
              <a href="https://astral.sh/" target="_blank">Astral (uv)</a>.
            </p>
            <p><strong>External models:</strong> Ultralytics YOLOv10 variants (N/S/M/B/L/X).</p>
            <p>
              <strong>Internal code:</strong> Simulation + battery model, oracle planner,
              training-data generation, imitation-learning controller, and profiling utilities.
            </p>
            <p>
              <strong>AI Coding Tools:</strong>
              <a href="https://windsurf.com/" target="_blank">Windsurf</a>,
              <a href="https://github.com/anthropics/claude-code" target="_blank">Claude Code</a>,
              <a href="https://chatgpt.com/" target="_blank">ChatGPT</a>
            </p>
          </section>
        </div>
      </main>
    </div>

    <script src="script.js"></script>
  </body>
</html>
